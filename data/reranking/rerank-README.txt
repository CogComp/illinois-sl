                            JLIS-Reranking
                            ==============

Author: Ming-Wei Chang <illinois-ml-nlp-users@cs.uiuc.edu>
Date: 2011-09-08 Thu



Table of Contents
=================
1 Why Reranking? 
2 Download! 
3 Contact 
4 How to train? 
5 How to test? 
6 How to do cross-validation? 
7 Providing training data 
8 Training/Testing file format 
9 (*IMPORTANT*) Clarification 
10 Reranking with indirect signals 
11 Bias Feature 
12 HELP! 
13 Getting Weight Vector 
14 Speed Concerns 
15 How to cite our package: Please cite 
16 Have Fun! 


1 Why Reranking? 
-----------------

  *This is a package that you can use it directly. No coding is required.*

  When applying structure learning techniques in practice, for the
  sake of efficiency, usually only local features are used. However,
  it is widely known that "long distance" relationships are important
  in many different cases. For example, in the task of co-reference
  resolution, you might want to use "entity-based" features, while
  your original structured classifier can only capture the local
  relationships (the relationships between two mentions).

  Discriminative Reranking is a technique proposed by Collins,
  M. (2000) to address this issue. The testing procedure has two
  steps:

  1. For each input x, use your baseline classifier to generate the TOP-K candidates y_1,y_2,...y_K.

  2. Use a reranker (which captures the long distance features) to
     rerank the candidates and generate the final output.

  The goal of the JLIS-reranking package is to provide an easy way to
  build a reranker using *structural SVM* (to be used in step 2).

  JLIS-rerank (based on [JLIS]) allows you to tune the regularization
  parameter C easily (via cross validation) and its learning takes
  into account of the performance measure (e.g. F1) of each candidate
  (hence, it performs "weighted" reranking).


  [JLIS]: file:indirect.html

4 How to train? 
----------------
* Example 
  
  ./jlis-rerank.sh trainReranker rerank.train 0.1 10
  
   It means that we want to train a reranker using the training data
   rerank.train with C=0.1 and 10 threads. The model will be saved to
   rerank.train.ssvm.model.
  
   Also use the following command for more help
  ./jlis-rerank.sh trainReranker
  
  

5 How to test? 
---------------
  
* Evaluation 
  The "labels" in reranking is the performance measure (for example, F1
  score). Therefore, at test time, we want to evaluate the reranker to
  see if it can pick the candidate with the best (highest) performance
  measure. To evaluate a reranker, we directly evaluate the performance
  measure of the chosen candidates for all examples.
  
  Hence, we will output the following score to evaluate our reranker
  
  \sum_i performance(F(x_i)) / l,
  
  where l represents the number of testing example, F(x_i) represents
  the candidate selected by our reranker, and performance(.) is a
  function that return the corresponding performance measure for a
  candidate.
  
  
* Example 
  
  ./jlis-rerank.sh testReranker rerank.train.ssvm.model rerank.test o
  
  
  
   It means that you want to test the "rerank.train.ssvm.model" on the
   test data "rerank.test" and output the results in "o".  The results
   (stored in "o") will have l lines, where each line corresponds to
   an example in the test data. If the line is "0", it means that the
   reranker chose the first candidate for the corresponding input
   example.  If the line is "2", it means that the reranker chose the
   third candidate for the corresponding input example.
  
   The average performance measure will also be printed on the screen:
  
  TOP1 Average Performance Measurement (no reranking, just pick the first one)
  0.68174368093597
  Reranked Average Performance Measurement (with reranking)
  0.7090422556883264
  
  
   The first number shows the performance of always picking the first
   candidate. In our example, the first candidate was the best tree
   generated by the Charniak parser. Our reranker improves the
   performance by 0.02.
  

7 Providing training data 
--------------------------

  We have just discussed the testing procedure using the reranking
  technique. However, how to train a reranker? More specifically, how
  to build a training data for training a reranker? (In
  order to use this package, the users need to
  generate the training data for training a reranker by themselves.)

  There are many ways to generate the training examples for a
  reranker. We describe one way of training a reranker in the
  following. A better way is to use cross-validation (please refer to
  Collins, M. (2000)). Other ways are also possible.
  
  + The held-out set approach 
    
    1. Split your training data into two parts: A and B.
    
    2. Train your baseline classifier using A
    
    3. Apply the trained baseline classifier on B and generate top-K candidates for each example in B
    
    4. Use the labels in B to decide the performance measure (for example, F1 score)
       of each candidate generated by the baseline classifier. (Note
       that the gold assignment might not appear in your
       candidates. Nevertheless, you can still use the gold labels to
       computer the performance measure)
    
    5. Use the performance measure (from step 4) and the candidates (from step 3) to train a reranker
       
    

8 Training/Testing file format 
-------------------------------

Note that in the training/testing data, each example should have the following information.

   1. The number of candidates this example has.

   2. The example id. The classifier will not use this information, we put it here
      because some users found it useful to have a id here.

   3. The performance measure of each candidate. The performance
      measure is calculated by using the gold (or indirect)
      labels. In some sense, this represents the "labeling
      information" in this data. For example, in the case of rerank
      parse trees, the performance measure can be the (micro)-F1 score
      of this candidate tree with respect to the gold tree.

   4. The name of each candidate. The classifier will not use this information. 

   5. The feature vector of each candidate


In the beginning of each example, the first token is the number of
candidate and it follows by the example id. Assume you have K
candidates. Then there will be 2K lines following the first line.  Each
candidate will have two lines. The first line has the performance measure followed
by the name of the candidate.  The second line represents the feature
vector of this candidate.

One example is in the following:

  6 ONTO_BN:nbc_0006.v2_gold_conll:0
  1.0 CHARNIAK_0
  1:1.0 2:1.0 3:2.0 4:1.0 5:1.0 6:1.0 7:2.0 8:1.0 9:1.0 10:1.0 11:2.0 12:2.0 13:1.0 14:1.0 15:1.0 16:0.0 17:1.0 18:1.0 19:1.0 20:1.0 21:1.0 22:1.0 23:1.0 24:1.0 
  0.0 CHARNIAK_47
  11:2.0 16:1.436688695439944 18:1.0 23:1.0 24:1.0 25:1.0 26:2.0 27:2.0 28:1.0 29:1.0 30:1.0 31:1.0 32:1.0 33:2.0 34:2.0 35:1.0 36:1.0 37:1.0 38:1.0 39:1.0 40:1.0 
  0.6666666666666666 CHARNIAK_51
  7:1.0 16:1.5279872428613663 22:1.0 23:1.0 41:1.0 42:1.0 43:1.0 44:1.0 45:1.0 46:1.0 47:1.0 48:1.0 49:1.0 50:1.0 51:1.0 52:1.0 53:1.0 
  0.0 CHARNIAK_80
  11:2.0 16:1.7448339274693199 18:1.0 23:1.0 24:1.0 25:1.0 26:3.0 27:2.0 28:0.2612038749637414 29:2.0 30:1.0 31:2.0 32:2.0 33:2.0 34:2.0 35:1.0 36:1.0 37:1.0 38:1.0 39:1.0 40:1.0 54:1.0 55:1.0 56:1.0 57:1.0 58:1.0 59:1.0 60:1.0 
  0.0 CHARNIAK_88
  6:1.0 7:1.0 10:1.0 15:1.0 16:1.8081317153816368 22:1.0 23:1.0 31:1.0 32:1.0 41:1.0 48:1.0 49:1.0 61:1.0 62:1.0 63:1.0 64:1.0 65:1.0 66:1.0 67:1.0 68:1.0 69:1.0 70:1.0 71:1.0 72:1.0 73:1.0 
  0.5 CHARNIAK_95
  6:1.0 11:2.0 16:1.8460432777910596 18:1.0 23:1.0 24:1.0 25:1.0 26:2.0 27:2.0 28:0.585786437626905 29:1.0 30:1.0 33:2.0 34:2.0 35:1.0 36:1.0 37:1.0 38:1.0 40:1.0 74:1.0 75:1.0 76:1.0 77:1.0


It represents that we have 6 candidates for this example. The
candidate names are `CHARNIAK_0', `CHARNIAK_47', `CHARNIAK_80', `CHARNIAK_88'
and `CHARNIAK_95', respectively. Their performance measure are 1.0, 0.0,
0.666666666666,0.0,0.0 and 0.5, respectively. Line 3,5,7,9,11 and 13
of the examples represent their corresponding feature vectors.

The performance measure is the "label" here, because it tells us how
good a candidate is. If the performance measure is higher, it means
the results are better. Take a look in rerank train for more examples.


