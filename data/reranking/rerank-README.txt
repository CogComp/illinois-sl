                            JLIS-Reranking
                            ==============

Author: Ming-Wei Chang <illinois-ml-nlp-users@cs.uiuc.edu>
Date: 2011-09-08 Thu



Table of Contents
=================
1 Why Reranking? 
2 Download! 
3 Contact 
4 How to train? 
5 How to test? 
6 How to do cross-validation? 
7 Providing training data 
8 Training/Testing file format 
9 (*IMPORTANT*) Clarification 
10 Reranking with indirect signals 
11 Bias Feature 
12 HELP! 
13 Getting Weight Vector 
14 Speed Concerns 
15 How to cite our package: Please cite 
16 Have Fun! 


1 Why Reranking? 
-----------------

  *This is a package that you can use it directly. No coding is required.*

  When applying structure learning techniques in practice, for the
  sake of efficiency, usually only local features are used. However,
  it is widely known that "long distance" relationships are important
  in many different cases. For example, in the task of co-reference
  resolution, you might want to use "entity-based" features, while
  your original structured classifier can only capture the local
  relationships (the relationships between two mentions).

  Discriminative Reranking is a technique proposed by Collins,
  M. (2000) to address this issue. The testing procedure has two
  steps:

  1. For each input x, use your baseline classifier to generate the TOP-K candidates y_1,y_2,...y_K.

  2. Use a reranker (which captures the long distance features) to
     rerank the candidates and generate the final output.

  The goal of the JLIS-reranking package is to provide an easy way to
  build a reranker using *structural SVM* (to be used in step 2).

  JLIS-rerank (based on [JLIS]) allows you to tune the regularization
  parameter C easily (via cross validation) and its learning takes
  into account of the performance measure (e.g. F1) of each candidate
  (hence, it performs "weighted" reranking).


  [JLIS]: file:indirect.html

2 Download! 
------------

  Get it here: [jlis-rerank-%s.tgz]


  [jlis-rerank-%s.tgz]: file:jlis-rerank-%s.tgz

3 Contact 
----------

  Note that this is the first release of JLIS-reranking, so there definitely is room to improve.

  Please contact [Ming-Wei Chang] with any bug reports, comments or questions.


  [Ming-Wei Chang]: illinois-ml-nlp-users@cs.uiuc.edu

4 How to train? 
----------------
* Example 
  
  ./jlis-rerank.sh trainReranker rerank.train 0.1 10
  
   It means that we want to train a reranker using the training data
   rerank.train with C=0.1 and 10 threads. The model will be saved to
   rerank.train.ssvm.model.
  
   Also use the following command for more help
  ./jlis-rerank.sh trainReranker
  
  

5 How to test? 
---------------
  
* Evaluation 
  The "labels" in reranking is the performance measure (for example, F1
  score). Therefore, at test time, we want to evaluate the reranker to
  see if it can pick the candidate with the best (highest) performance
  measure. To evaluate a reranker, we directly evaluate the performance
  measure of the chosen candidates for all examples.
  
  Hence, we will output the following score to evaluate our reranker
  
  \sum_i performance(F(x_i)) / l,
  
  where l represents the number of testing example, F(x_i) represents
  the candidate selected by our reranker, and performance(.) is a
  function that return the corresponding performance measure for a
  candidate.
  
  
* Example 
  
  ./jlis-rerank.sh testReranker rerank.train.ssvm.model rerank.test o
  
  
  
   It means that you want to test the "rerank.train.ssvm.model" on the
   test data "rerank.test" and output the results in "o".  The results
   (stored in "o") will have l lines, where each line corresponds to
   an example in the test data. If the line is "0", it means that the
   reranker chose the first candidate for the corresponding input
   example.  If the line is "2", it means that the reranker chose the
   third candidate for the corresponding input example.
  
   The average performance measure will also be printed on the screen:
  
  TOP1 Average Performance Measurement (no reranking, just pick the first one)
  0.68174368093597
  Reranked Average Performance Measurement (with reranking)
  0.7090422556883264
  
  
   The first number shows the performance of always picking the first
   candidate. In our example, the first candidate was the best tree
   generated by the Charniak parser. Our reranker improves the
   performance by 0.02.
  

6 How to do cross-validation? 
------------------------------
  ./jlis-rerank.sh crossValidationReranker rerank.train 1 2 5


It means that you want to do 5-fold cross-validation with C=1 and 2
threads on rerank.train. In fact, it is fairly important to pick C in
our example.

The cross-validation performance measure will also be printed on the screen.

Also use the following command to get more help
  ./jlis-rerank.sh crossValidationReranker



7 Providing training data 
--------------------------

  We have just discussed the testing procedure using the reranking
  technique. However, how to train a reranker? More specifically, how
  to build a training data for training a reranker? (In
  order to use this package, the users need to
  generate the training data for training a reranker by themselves.)

  There are many ways to generate the training examples for a
  reranker. We describe one way of training a reranker in the
  following. A better way is to use cross-validation (please refer to
  Collins, M. (2000)). Other ways are also possible.
  
  + The held-out set approach 
    
    1. Split your training data into two parts: A and B.
    
    2. Train your baseline classifier using A
    
    3. Apply the trained baseline classifier on B and generate top-K candidates for each example in B
    
    4. Use the labels in B to decide the performance measure (for example, F1 score)
       of each candidate generated by the baseline classifier. (Note
       that the gold assignment might not appear in your
       candidates. Nevertheless, you can still use the gold labels to
       computer the performance measure)
    
    5. Use the performance measure (from step 4) and the candidates (from step 3) to train a reranker
       
    

8 Training/Testing file format 
-------------------------------

Note that in the training/testing data, each example should have the following information.

   1. The number of candidates this example has.

   2. The example id. The classifier will not use this information, we put it here
      because some users found it useful to have a id here.

   3. The performance measure of each candidate. The performance
      measure is calculated by using the gold (or indirect)
      labels. In some sense, this represents the "labeling
      information" in this data. For example, in the case of rerank
      parse trees, the performance measure can be the (micro)-F1 score
      of this candidate tree with respect to the gold tree.

   4. The name of each candidate. The classifier will not use this information. 

   5. The feature vector of each candidate


In the beginning of each example, the first token is the number of
candidate and it follows by the example id. Assume you have K
candidates. Then there will be 2K lines following the first line.  Each
candidate will have two lines. The first line has the performance measure followed
by the name of the candidate.  The second line represents the feature
vector of this candidate.

One example is in the following:

  6 ONTO_BN:nbc_0006.v2_gold_conll:0
  1.0 CHARNIAK_0
  1:1.0 2:1.0 3:2.0 4:1.0 5:1.0 6:1.0 7:2.0 8:1.0 9:1.0 10:1.0 11:2.0 12:2.0 13:1.0 14:1.0 15:1.0 16:0.0 17:1.0 18:1.0 19:1.0 20:1.0 21:1.0 22:1.0 23:1.0 24:1.0 
  0.0 CHARNIAK_47
  11:2.0 16:1.436688695439944 18:1.0 23:1.0 24:1.0 25:1.0 26:2.0 27:2.0 28:1.0 29:1.0 30:1.0 31:1.0 32:1.0 33:2.0 34:2.0 35:1.0 36:1.0 37:1.0 38:1.0 39:1.0 40:1.0 
  0.6666666666666666 CHARNIAK_51
  7:1.0 16:1.5279872428613663 22:1.0 23:1.0 41:1.0 42:1.0 43:1.0 44:1.0 45:1.0 46:1.0 47:1.0 48:1.0 49:1.0 50:1.0 51:1.0 52:1.0 53:1.0 
  0.0 CHARNIAK_80
  11:2.0 16:1.7448339274693199 18:1.0 23:1.0 24:1.0 25:1.0 26:3.0 27:2.0 28:0.2612038749637414 29:2.0 30:1.0 31:2.0 32:2.0 33:2.0 34:2.0 35:1.0 36:1.0 37:1.0 38:1.0 39:1.0 40:1.0 54:1.0 55:1.0 56:1.0 57:1.0 58:1.0 59:1.0 60:1.0 
  0.0 CHARNIAK_88
  6:1.0 7:1.0 10:1.0 15:1.0 16:1.8081317153816368 22:1.0 23:1.0 31:1.0 32:1.0 41:1.0 48:1.0 49:1.0 61:1.0 62:1.0 63:1.0 64:1.0 65:1.0 66:1.0 67:1.0 68:1.0 69:1.0 70:1.0 71:1.0 72:1.0 73:1.0 
  0.5 CHARNIAK_95
  6:1.0 11:2.0 16:1.8460432777910596 18:1.0 23:1.0 24:1.0 25:1.0 26:2.0 27:2.0 28:0.585786437626905 29:1.0 30:1.0 33:2.0 34:2.0 35:1.0 36:1.0 37:1.0 38:1.0 40:1.0 74:1.0 75:1.0 76:1.0 77:1.0


It represents that we have 6 candidates for this example. The
candidate names are `CHARNIAK_0', `CHARNIAK_47', `CHARNIAK_80', `CHARNIAK_88'
and `CHARNIAK_95', respectively. Their performance measure are 1.0, 0.0,
0.666666666666,0.0,0.0 and 0.5, respectively. Line 3,5,7,9,11 and 13
of the examples represent their corresponding feature vectors.

The performance measure is the "label" here, because it tells us how
good a candidate is. If the performance measure is higher, it means
the results are better. Take a look in rerank train for more examples.

9 (*IMPORTANT*) Clarification 
------------------------------

It is very important to note the differences between "performance
measure" and "activation". Some people might want to put the
activation/probability of your baseline classifier in the field of
performance measure. This is *wrong*. Performance measure should
ALWAYS be calculated by using gold (or indirect) labeled data in order
to make the reranker meaningful.

On the other hand, it is a good idea to order your candidates using
the activation scores of baseline classifier (e.g. put the one with
the highest activation from the baseline classifier as the first
candidate) so that it would be clear whether if the reranker beats
your baseline classifier or not.

10 Reranking with indirect signals 
-----------------------------------

It is also important to understand that the performance measure
sometimes can come from other indirect supervision signals, this makes
the JLIS-reranking a wonderful playground for testing different
indirect supervision signals.

11 Bias Feature 
----------------

  There is no need to add a bias term in the reranker.

12 HELP! 
---------

  In order to get help for the commands, type "./jlis-rerank.sh". It will list
  all of the commands and their usage. You can also only look at the
  usage of one command. For example,

  >./jlis-rerank.sh trainReranker
  
  trainReranker String String String 
  <DESCRIPTION>
          This procedure is used to train a reranker model using ssvm 
  <INPUT>
          It receives 3 different arguments. 
          1) train_file (a string), the file name of the training data 
          2) C (a real number > 0), a regularization parameter. 
          3) n_thread (an integer), which indicates how many thread you want to use (you do not want to use more threads than the number of cores you have in your computer). 
  <OUTPUT>
          The trained model file will be saved to ${train_file}.ssvm.model in the current working directory.



13 Getting Weight Vector 
-------------------------

  The model is currently saved using java serialization, so you can not see the weight vector directly. One might ask, what happen if I want to see the weight vectors?

* Example: Output Weight Vector 
  
  No problem. The following command outputs the contents of the weight vector.
  
    ./jlis-rerank.sh outputWeightVector rerank.train.ssvm.model w
  
  
  It means that you want to extract the weight vectors inside "rerank.train.ssvm.model" and put it in "w"
  
  
  

14 Speed Concerns 
------------------

  Note that while we have the option to specify number of threads you
  want to use to train a model, the improvement over the number of
  thread is not linear. The reason is that only part of the learning
  code can take advantage of multiple cores. 

  Also, the number of threads you specify should equal to or less than
  the number of cores your computer has. 

15 How to cite our package: Please cite 
----------------------------------------

Ming-Wei Chang, Vivek Srikumar, Dan Goldwasser and Dan
Roth. Structured output learning with indirect supervision. In
Proc. of the International Conference on Machine Learning (ICML),
2010.

16 Have Fun! 
-------------

  Have Fun! Please let us (illinois-ml-nlp-users@cs.uiuc.edu) know if your successful/unsuccessful stories. 
  
  
