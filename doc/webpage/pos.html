<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>POS Tutorial</title>
<!-- 2015-05-06 Wed 10:30 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="Shyam Upadhyay" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">POS Tutorial</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Sentence</a></li>
<li><a href="#sec-2">POS Tag</a></li>
<li><a href="#sec-3">Features</a></li>
<li><a href="#sec-4">Parameters</a></li>
<li><a href="#sec-5">Reading the Training Data</a></li>
<li><a href="#sec-6">Training</a></li>
<li><a href="#sec-7">Testing</a></li>
<li><a href="#sec-8">Final Remarks</a></li>
</ul>
</div>
</div>
{% raw %}

\(

\newcommand{\opt}[1]{{#1}^{*}} 
\newcommand{\pred}[1]{\hat{#1}} 
\newcommand{\dnorm}[1]{{#1}^{*}} 
\newcommand{\dotprod}[2]{ {#1}^T{#2}}
\newcommand{\dotproduct}[2]{ \langle {#1},{#2} \rangle }
\newcommand{\Forall}{\,\forall\,}
\renewcommand{\Pr}{\mathbb{P}} 
\renewcommand{\vec}[1]{\mathbf{#1}} 
\renewcommand{\ceiling}[1]{\lceil {#1} \rceil}

\DeclareMathOperator*{\argmin}{\mathbf{arg\,min}}
\DeclareMathOperator*{\argmax}{\mathbf{arg\,max}}
\DeclareMathOperator*{\sup}{sup}
\DeclareMathOperator{\F}{\mathscr{F}} 
\DeclareMathOperator{\H}{\mathscr{H}} 
\DeclareMathOperator{\R}{\mathbb{R}} 
\DeclareMathOperator{\E}{\mathbb{E}} 
\DeclareMathOperator{\Or}{\mathcal{O}}
\DeclareMathOperator{\Tr}{\textbf{Tr}} 
\DeclareMathOperator{\grad}{\nabla} 
\DeclareMathOperator{\LLH}{\mathcal{L}} 
\DeclareMathOperator{\Lag}{\mathcal{L}} 
\DeclareMathOperator{\X}{\mathcal{X}} 
\DeclareMathOperator{\Y}{\mathcal{Y}} 
\DeclareMathOperator{\w}{\mathbf{w}} 
\DeclareMathOperator{\bF}{\mathbf{F}} 
\DeclareMathOperator{\y}{\mathbf{y}} 
\DeclareMathOperator{\x}{\mathbf{x}} 


\)

{% endraw %}
<p>
We will start with a very simple example - part-of-speech tagging. 
</p>

<p>
We will work under the usual markovian assumption, so the POS tags will be the hidden states. We will have features based on transistion probabilities, emission probabilities and the initial probabilities for each state.
</p>

<p>
We will define our loss to be the hamming distance between the predicted POS tags for the sentence and the gold POS tag sequence for the sentence. Note that this loss decomposes over the structure, so we can use the same inference solver to solve loss-augmented inference and the MAP inference problem.
</p>

<p>
For the case of POS Tagging, the components that we need to implement are,
</p>
<ol class="org-ol">
<li>The sentence (this will be the \(\x\)).
</li>
<li>The POS tags for the sentence (this will be the \(\y\)).
</li>
<li>Our features will encode the transistion probabilities, emission probabilities and the initial state probabilities.
</li>
<li>An inference solver. We will use the Viterbi Inference method. 
</li>
</ol>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Sentence</h2>
<div class="outline-text-2" id="text-1">
<p>
For POS Tagging , sentence represents the input structure. Each token in the sentence will be part of the structure. 
</p>
<pre class="example">
public class Sentence implements IInstance {
    public final int[] tokens;
}
</pre>
<p>
For each token in the sentence, we just store the id of the token maintained in the lexicon.
Note that all input structures should implement the <code>IInstance</code> interface.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">POS Tag</h2>
<div class="outline-text-2" id="text-2">
<p>
The POS tag sequence represents the output structure. 
</p>
<pre class="example">
public class POSTag implements IStructure {
    public final int[] tags;
    public POSTag(int[] tags){
        this.tags = tags;
    }
}
</pre>
<p>
Again, we just store the ids, as maintained by the lexicon. Note that all output structures should implement <code>IStructure</code> interface.
</p>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Features</h2>
<div class="outline-text-2" id="text-3">
<p>
We are going to index all the features into the same feature vector, offseted by the maximum size of each feature type (template). 
</p>

<p>
For example, suppose the maximum number of possible type A features are 100, maximum number of possible type B features are 200, and maximum number of possible type C features are 250. Then index [0,100] will hold the type A features, [101,300] will hold the type B features and [301,550] will hold type C features.
</p>

<pre class="example">
public IFeatureVector getFeatureVector(IInstance x, IStructure y) {
    FeatureVectorBuffer fv = new FeatureVectorBuffer();
    Sentence ins = (Sentence) x;
    int[] tags = ((POSTag) y).tags;

    // add emission features
    for (int i = 0; i &lt; ins.tokens.length; i++)
        fv.addFeature(ins.tokens[i] +  lm.getNumOfFeature() * tags[i], 1.0f);

    // add prior features 
    fv.addFeature(lm.getNumOfFeature() * lm.getNumOfLabels() + tags[0], 1.0f);

    // add transition features
    int priorEmissionOffset = lm.getNumOfFeature() * lm.getNumOfLabels() + lm.getNumOfLabels();
    // calculate transition features
    for (int i = 1; i &lt; ins.tokens.length; i++)
        fv.addFeature(priorEmissionOffset + tags[i - 1] * lm.getNumOfLabels() + tags[i], 1.0f);

    return fv.toFeatureVector(); 
}
</pre>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Parameters</h2>
<div class="outline-text-2" id="text-4">
<p>
  We will use a Structured Perceptron for learning the model, so we need to specify relevant parameters.
The parameters can be specified in a config file, which can then load into a <code>SLParameters</code> object. You can find the config file in the package, under the <code>config/</code> directory. We also need to set the total numbers of features that we are going to see during training,
</p>
<pre class="example">
SLParameters para = new SLParameters();
para.loadConfigFile(configFilePath);
para.TOTAL_NUMBER_FEATURE = model.lm.getNumOfFeature()*model.lm.getNumOfLabels()+model.lm.getNumOfLabels()+model.lm.getNumOfLabels()*model.lm.getNumOfLabels();
</pre>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Reading the Training Data</h2>
<div class="outline-text-2" id="text-5">
<p>
We will assume the training data is in the following format,
</p>
<pre class="example">
I ate an apple .
PRP VBD DT NN .
The quick brown fox jumped over the lazy dog .
DT JJ JJ NN VBD IN DT JJ NN .
</pre>
<p>
First the tokens of the sentence are listed, followed by the POS tags for each token.
</p>

<p>
As we read the data, we will put the words into the lexicon. 
</p>
<pre class="example">
for (int j = 0; j &lt; words.length; j++) {
    // this will be off at test time, so new words wont be added to the lexicon
    if (lm.isAllowNewFeatures()) {
    lm.addFeature("w:" + words[j]);
}
if (lm.containFeature("w:" + words[j]))
    wordIds[j] = lm.getFeatureId("w:" + words[j]);
else
    wordIds[j] = lm.getFeatureId("W:unknownword"); 
    // new word seen at test time, so we map it to unknown.
    // Note that else condition will only occur at test time.
}
Sentence x = new Sentence(wordIds);
</pre>

<p>
Similarly, we read the POS tag sequence, adding the labels to the lexicon,
</p>
<pre class="example">
for (int j = 0; j &lt; tags.length; j++) {
    lm.addLabel("tag:" + tags[j]);
    tagIds[j] = lm.getLabelId("tag:" + tags[j]);
}
POSTag y = new POSTag(tagIds);
</pre>
<p>
For ease of access, we put both the sentence token ids and the tags ids in the <code>SLProblem</code> object. The <code>SLProblem</code> class is a wrapper around a list of IInstance and the correponding gold IStructure objects. We add the sentence and POSTag to the <code>SLProblem</code> object,
</p>
<pre class="example">
sp.addExample(x, y);
</pre>

<p>
Here is the entire code for reading the data,
</p>
<pre class="example">
public static SLProblem readStructuredData(String fname, Lexiconer lm)
throws IOException, DataFormatException {
    List&lt;String&gt; lines = LineIO.read(fname);
    SLProblem sp = new SLProblem();

    assert lines.size() % 2 == 0; // must be even; contains labels

    if (lm.isAllowNewFeatures())
        lm.addFeature("W:unknownword"); 
        // pre-add the unknown word to the vocab

    for (int i = 0; i &lt; lines.size() / 2; i++) {
        String[] words = lines.get(i * 2).split("\\s+");
        int[] wordIds = new int[words.length];

        for (int j = 0; j &lt; words.length; j++) {
        // this will be off at test time, so new words wont be added to the lexicon
            if (lm.isAllowNewFeatures()) {
                lm.addFeature("w:" + words[j]);
            }
            if (lm.containFeature("w:" + words[j]))
                wordIds[j] = lm.getFeatureId("w:" + words[j]);
            else
                wordIds[j] = lm.getFeatureId("W:unknownword");
                // new word seen at test time, so we map it to unknown
        }
        Sentence x = new Sentence(wordIds);
        String[] tags = lines.get(i * 2 + 1).split("\\s+");
        int[] tagIds = new int[words.length];

        if (words.length != tags.length) {
            throw new DataFormatException(
            "The number of tokens and number tags in " + i
            + "-th sample does not match");
        }
        for (int j = 0; j &lt; tags.length; j++) {
            lm.addLabel("tag:" + tags[j]);
            tagIds[j] = lm.getLabelId("tag:" + tags[j]);
        }
        sp.addExample(x, new POSTag(tagIds));
    }
    return sp;
}
</pre>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Training</h2>
<div class="outline-text-2" id="text-6">
<p>
First, we create a new <code>SLModel</code> object that will contain all the information relevant to our model.
In particular, it contains,
</p>
<ol class="org-ol">
<li>The Lexicon - the lexicon manages the mapping from your features to ids and labels to ids. The lexicon is implemented in the class <code>Lexiconer</code>.
</li>
<li>The weight vector for your model.  
</li>
<li>Inference Solver - the method that you use to solve 
</li>
</ol>

<pre class="example">
SLModel model = new SLModel();
model.lm = new Lexiconer();

SLProblem sp = readStructuredData(trainingDataPath, model.lm);
</pre>

<p>
Now lets read the training data for POS tagging.
We are going to populate the lexicon as we read the data. 
Once you have read all the data, make sure to switch off the addition of new features by calling,
</p>
<pre class="example">
// Disallow the creation of new features
model.lm.setAllowNewFeatures(false);
</pre>
<p>
<b>This is important</b>. If you forget to do this, the lexicon will keep on adding new words to the vocabulary during test time!
</p>


<p>
When you are done training, you can save your model by calling the <code>saveModel</code> method in the <code>SLModel</code> class.
</p>
</div>
</div>

<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7">Testing</h2>
<div class="outline-text-2" id="text-7">
<p>
Testing is as easy as training. We simply load the <code>SLModel</code> object that we obtained from training and read the test data into a <code>SLProblem</code>. We access each test instance using the <code>instanceList</code> field in the <code>SLProblem</code> object, and output the best structure as our prediction.
</p>
<pre class="example">
SLModel model = SLModel.loadModel(modelPath);
SLProblem sp = readStructuredData(testDataPath, model.lm);

double acc = 0.0;
double total = 0.0;

for (int i = 0; i &lt; sp.instanceList.size(); i++) {

    POSTag gold = (POSTag) sp.goldStructureList.get(i);
    POSTag prediction = (POSTag) model.infSolver.getBestStructure(model.wv, sp.instanceList.get(i));

    for (int j = 0; j &lt; prediction.tags.length; j++) {
        total += 1.0;
        if (prediction.tags[j] == gold.tags[j]) {
        acc += 1.0;
    }
}
</pre>
</div>
</div>
<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8">Final Remarks</h2>
<div class="outline-text-2" id="text-8">
<p>
You can find the source code for the POS tagging and other examples under <code>src/main/java/edu/illinois/cs/cogcomp/sl/applications/tutorial</code>
in the downloaded package.
You can also find a simple script to run the POS tagging example at <code>scripts/run_tutorial.sh</code>.
It will train your model, and test it on a toy dataset.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Shyam Upadhyay</p>
<p class="date">Created: 2015-05-06 Wed 10:30</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.2.6)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
